---
title: "IAL-620 Project"
author: "Ian Skarring"
date: "12/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exploratory Analysis of LessWrong Essay Sequence

This is a report for my course project in natural language processing and text mining, where I am doing analytics of a selected corpus of texts. For my corpus, I have scraped sequences of blog posts from Less Wrong, where Eliezer Yudkowsky, Scott Alexander, and others write about artificial intelligence, behavioral economics, rationality, neuroscience, and human decision making from theoretical perspectives. Eliezer is well-known for presenting ideas about friendly AI, and founded the Machine Intelligence Research Institute. For this exploratory analysis I will focus on the [Argument and Analysis](https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi/) sequence by [Scott Alexander](https://twitter.com/slatestarcodex).

**Key points / links:** 

* [Less Wrong](https://www.lesswrong.com/)

* [Machine Intelligence Research Institute](https://intelligence.org/)

```{r echo = FALSE, warning = FALSE, message = FALSE}

#loading packages
library(ggraph)
library(ggplot2)
library(igraph)
library(ldatuning)
library(lubridate)
library(rvest)
library(stringr)
library(text2vec)
library(tidytext)
library(tidyverse)
library(tm)
library(topicmodels)
library(widyr)

#scrape webpages for text
#url object
main_url <- read_html('https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi')

#getting the links
hrefs <- html_nodes(main_url, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "PostsItem2-title", " " ))]') %>%
  html_children() %>%
  html_children() %>%
  html_children() %>%
  html_attr('href')

#removing NAs
hrefs <- na.omit(hrefs)

#pasting hrefs to root url to create links
#empty vector for loop
links <- vector()

#looping for each href
for (i in seq_along(hrefs)) {
  links[i] <- paste0('https://www.lesswrong.com', hrefs[i])
}
#View(links)

#cleaning up work space
remove(hrefs, i)

#text and date objects
text <- vector()
date <- vector()

#for loop to get text, date from pages 
for (i in seq_along(links)) {
  new_url <- read_html(links[i])

  text_data <- html_nodes(new_url, xpath = '//p') %>%
    html_text()
  
  text_data <- paste(text_data, collapse = ' ')
  
  date_data <- html_nodes(new_url, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "PostsPageDate-date", " " ))]//span') %>%
    html_text()
  
  text <- c(text, text_data)
  date <- c(date, date_data)
}

#View(links)
```

#### Tools and libraries

For scraping the sequences of essays I used the [rvest](https://cran.r-project.org/web/packages/rvest/rvest.pdf) library. Next to do basic text processing and cleaning I used the [tm](https://cran.r-project.org/web/packages/tm/tm.pdf), [tidyverse](https://www.tidyverse.org/packages/), and [tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html) libraries. This was helpful as I got to use the `%>%` operator in processing, which provides a clean and easy-to-interpret syntax.

For visualizations I used the popular [ggplot2](https://ggplot2.tidyverse.org/) library, which fits within the core Tidyverse and also extends the use of the `%>%` operator. Additionally, I used the [ggraph](https://cran.r-project.org/web/packages/ggraph/ggraph.pdf) and [igraph](https://igraph.org/r/) libraries to plot word and bigram correlations. 

For vectorizing text and creating LDA models, I used the [ldatuning](https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html), [text2vec](http://text2vec.org/), and [topicmodels](https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf) libraries.

#### About the author

[Scott Alexander](https://twitter.com/slatestarcodex) (pseudonym) is a psychiatrist in the United States who started writing essays on a variety of topics around cognition, human decision-making, behavioral economics, and psychoanalysis. This sequence follows these topics from an argumentation and discourse analysis perspective. Scott became very well known for work on a now archived blog [Slate Star Codex](https://slatestarcodex.com/). 

#### Data for this project

**Essays in Argument and Analysis Sequence**

* [Eight Short Studies On Excuses](https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi/p/gFMH3Cqw4XxwL69iy)
* [Schelling Fences On Slippery Slopes](https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi/p/Kbm6QnJv9dgWsPHQP)
* [Intellectual Hipsters and Meta-Contrarianism](https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi/p/9kcTNWopvXFncXgPy)
* [Cardiologists and Chinese Robbers](https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi/p/DSzpr8Y9299jdDLc9)
* [All Debates Are Bravery Debates](https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi/p/PQ3nutgxfTgvq69Xt)
* [The Virtue of Silence](https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi/p/2brqzQWfmNx5Agdrx)
* [Proving Too Much](https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi/p/G5eMM3Wp3hbCuKKPE)
* [Beware Isolated Demands For Rigor](https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi/p/fzeoYhKoYPR3tDYFT)
* [Transhumanist Fables](https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi/p/AYbhqi65SWzHzy7Xx)
* [And I Show You How Deep The Rabbit Hole Goes](https://www.lesswrong.com/s/XsMTxdQ6fprAQMoKi/p/wJnm5cBiZGmKn595f)

```{r echo = FALSE, warning = FALSE, message = FALSE}
#combine objects into tibble
data <- tibble('url' = links, 'date' = date, 'text' = text)

#cleaning up work space
remove(main_url, new_url, date, date_data, i, links, text, text_data)

#convert date format
data$date <- dmy(data$date)

#one-word-per-line format
text <- data %>%
  unnest_tokens(word, text)

```

#### Count of top 10 occurring words after removing stop words

```{r echo = FALSE, warning = FALSE, message = FALSE}

#top occurring words
#text %>%
#  count(word, sort = TRUE)

#remove stop words
data("stop_words")
text <- text %>%
  anti_join(stop_words)

#top occurring words again
#text %>%
#  count(word, sort = TRUE)

'%notin%' <- Negate('%in%')

text <- text %>%
  filter(word %notin% c("1"))

#text %>%
#  count(word, sort = TRUE)
```

Right away in some exploratory text analysis it is clear the sequence of essays is focused on argumentation and discourse in terms of word counts. The plot below shows word frequencies across the essays in the argument and analysis sequence. Many words such as *contrarian*, *position*, *argument*, *students*, *law*, and *true* point to rationality in discourse. It also seems that there could be some general themes such as human behavior, decision making, and cognition.

```{r echo = FALSE, warning = FALSE, message = FALSE}

#visual of top occurring words
text %>%
  count(word, sort = TRUE) %>%
  top_n(n = 15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = 'red')) +
  geom_col(show.legend = FALSE) +
  xlab('Word') +
  ylab('Number of occurrences') +
  labs(title = 'Top 15 occuring words') +
  coord_flip()

```

```{r echo = FALSE, warning = FALSE, message = FALSE}

#sentiment analysis
#negative sentiments object
bing_negative <- get_sentiments('bing') %>%
  filter(sentiment == 'negative')

#top negative sentiments
negatives <- text %>%
  inner_join(bing_negative) %>%
  count(word, sort = TRUE)

#positive sentiments object
bing_positive <- get_sentiments('bing') %>%
  filter(sentiment == 'positive')

#top positive sentiments
positives <- text %>%
  inner_join(bing_positive) %>%
  count(word, sort = TRUE)

```

Similarly with the top negative tokens there is a general theme with words that point towards discourse on complex, foggy topics. *False*, *wrong*, *excuse*, *bias*, *defect*, and *denial* suggest examples of poorly defended positions in arguments across the essays.  

```{r echo = FALSE, warning = FALSE, message = FALSE}

#visual of top negative tokens
negatives %>%
  filter(n >= 10) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = 'identity', fill = '#d48206') +
  xlab('Word') +
  ylab('Number of occurrences') +
  labs(title = 'Top Occurring Negative Tokens') +
  coord_flip()

```

```{r echo = FALSE, warning = FALSE, message = FALSE}
#visual of top positive tokens
positives %>%
  filter(n >= 8) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat = 'identity', fill = '#685d87') +
  xlab('Word') +
  ylab('Number of occurrences') +
  labs(title = 'Top Occurring Positive Tokens') +
  coord_flip()

```

#### How much do individual words contribute to sentiment?

```{r echo = FALSE, warning = FALSE, message = FALSE}

#word contributions to sentiment
word_contribution <- text %>%
  inner_join(get_sentiments('bing')) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

#visual of top word contributions
word_contribution %>%
  group_by(sentiment) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = 'free_y') +
  coord_flip() +
  labs(x = NULL, y = 'contribution to sentiment')

```

The plots above contain a few interesting points that illustrate well how important it is to have context in reading content. In the absence of other words, it is impossible to determine the meaning beyond the definition of the individual word itself. For example *bias* is recorded as a token with negative sentiment, but doesn't factor in if the text contains words that suggest bias leaning in one direction or another. The text could say something along the lines of the absence of bias, which could potentially be positive. *Free* and *worth* in the positive sentiments plot are also in a similar bin. When used with other words, free could convey both positive and negative sentiments. Free of harm is positive, while a free item could be written differently: time spent or time lost, which have negative sentiments. This could be said about all words, but I think it is important to recognize how important context is and how sentiment analysis can actually be somewhat misleading at times. 

#### Analysis of bigrams

```{r echo = FALSE, warning = FALSE, message = FALSE}
#bigram analysis
bigrams <- data %>%
  unnest_tokens(bigram, text, token = 'ngrams', n = 2)

#separate into two columns
bigrams_separate <- bigrams %>%
  separate(bigram, c('word1', 'word2'), sep = ' ')

#afinn lexicon object
afinn <- get_sentiments('afinn')

#example
more_words <- bigrams_separate %>%
  filter(word1 == 'more') %>%
  inner_join(afinn, by = c(word2 = 'word')) %>%
  count(word2, value, sort = TRUE)
#View(more_words)

#visual of top pretty bigrams
more_words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(15) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = 'Top Bigrams with "Pretty"',
       x = 'Bigrams starting with "Pretty"',
       y = 'Sentiment Contributions')

```

The above bigrams lead with *Pretty*, while the negative bigrams below are misleading. While in the positive bigrams leading with pretty, the second word is mapped to a corresponding sentiment that we could see as correct. In the negative bigrams however, when leading with words such as *never*, *no*, *not*, and *without*, the second word seems to be considered in isolation. For example, we could consider a bigram such as *never liked* as negative, but only the second term is recognized in terms of its sentiment. So more frequently across the negative bigrams the sentiment could be reversed.

```{r echo = FALSE, warning = FALSE, message = FALSE}

#common negative bigrams
negate_word <- c('not', 'no', 'never', 'without')

#count of negative bigrams
negate_bigrams <- bigrams_separate %>%
  filter(word1 %in% negate_word) %>%
  inner_join(afinn, by = c(word2 = 'word')) %>%
  count(word1, word2, value, sort = TRUE)


```

```{r echo = FALSE, warning = FALSE, message = FALSE}

#visual of top negative bigrams
negate_bigrams %>%
  mutate(contribution = n * value,
         word2 = reorder(paste(word2, word1, sep = '__'), contribution)) %>%
  group_by(word1) %>%
  top_n(10, abs(contribution)) %>%
  ggplot(aes(word2, contribution, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = 'free') +
  scale_x_discrete(labels = function(x) gsub('__.+$', '', x)) +
  coord_flip() +
  labs(title = 'Top Negative Bigrams',
       x = 'Negative bigrams',
       y = 'Sentiment Contributions')


```

```{r echo = FALSE, warning = FALSE, message = FALSE}
#cleaning up work space 
remove(afinn, bigrams, bigrams_separate, bing_negative, bing_positive,
       more_words, negate_bigrams, negatives, positives, stop_words, text,
       word_contribution, negate_word)
```

```{r echo = FALSE, warning = FALSE, message = FALSE}

#word frequency and n-gram correlation analysis
#cleaning text
data$text <- gsub('\r?\n|\r', ' ', data$text)
data$text <- gsub('[[:punct:]]', '', data$text)
data$text <- tolower(data$text)
data$text <- removeWords(data$text, stopwords('smart'))
data$text <- stripWhitespace(data$text)

#term-frequency matrix
words_by_url <- data %>%
  unnest_tokens(word, text) %>%
  count(url, word, sort = TRUE)

#determine token count per url
url_words <- words_by_url %>%
  group_by(url) %>%
  summarize(total = sum(n))

```


```{r echo = FALSE, warning = FALSE, message = FALSE}

#combine words_by_url and url_words objects
words_by_url <- left_join(words_by_url, url_words)
#View(words_by_url)

#binding tf and idf to tidy data
words_by_url <- words_by_url %>%
  bind_tf_idf(word, url, n)

#remove total and sort descending order
words_by_url <- words_by_url %>%
  select(-total) %>%
  arrange(desc(tf_idf))

tokens <- data %>%
  unnest_tokens(word, text)

```

#### Comparison of top words by URLs to all tokens

**Top words by URL**

```{r echo = FALSE, warning = FALSE, message = FALSE}
#compare top words by urls with all tokens
unique(words_by_url$word[1:25])
```

**Top tokens**

```{r echo = FALSE, warning = FALSE, message = FALSE}
tokens$word[1:25]

#bigram analysis
bigrams_by_url <- data %>%
  unnest_tokens(bigram, text, token = 'ngrams', n = 2)
```

**Top bigrams**

```{r echo = FALSE, warning = FALSE, message = FALSE}
#count and sort bigrams
bigrams_by_url %>%
  count(bigram, sort = TRUE)

```

#### Comparison of tokens, words by URL, and bigrams

```{r echo = FALSE, warning = FALSE, message = FALSE}

#separate bigrams into two columns
bigrams_separate <- bigrams_by_url %>%
  separate(bigram, c('word1', 'word2'), sep = ' ')

#binding tf and idf to tidy data, then sorting
bigrams_tfidf <- bigrams_by_url %>%
  count(url, bigram) %>%
  bind_tf_idf(bigram, url, n) %>%
  arrange(desc(tf_idf))
```

**Tokens**

```{r echo = FALSE, warning = FALSE, message = FALSE}
#compare tokens, words by url, and bigrams
tokens$word[1:25]
```

**Top words by URL**

```{r echo = FALSE, warning = FALSE, message = FALSE}
unique(words_by_url$word[1:25])

```

**Top bigrams from tf_idf**

```{r echo = FALSE, warning = FALSE, message = FALSE}
bigrams_tfidf$bigram[1:25]

```

In the list of bigrams above from the term frequency-inverse document frequency matrix we can see the terms provide some more context on the essays in the sequence. In a sequence focusing on rationality, argumentation, and analysis it makes sense to see some topics of more contention. In particular, *argument proves*, *argument disproven*, *disprove existence*, and *creates expectation* all point towards more rational approaches to discourse.

**Bigram relationships**

```{r echo = FALSE, warning = FALSE, message = FALSE}

#visual of bigram relationships
bigrams_count <- function(data) {
  data %>%
    unnest_tokens(bigram, text, token = 'ngrams', n = 2) %>%
    separate(bigram, c('word1', 'word2'), sep = ' ') %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

bigrams_visual <- function(bigrams) {
  set.seed(1234)
  x <- grid::arrow(type = 'closed', length = unit(0.15, 'inches'))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = 'fr') +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = x) +
    geom_node_point(color = 'red', size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}

visual <- data %>%
  bigrams_count()

visual %>%
  filter(n > 5) %>%
  bigrams_visual()
```

The plot above shows frequently occurring bigrams, or frequently co-occurring words. Many seem to be what we could describe as typical bigrams, ones that occur across sequences of text in many domains: *medical confidentiality*, *slippery slope*, *brute strength*, *global warming*, *makes sense*, and *minimum wage*. The frequent occurrence of *accepting excuse* fits in with the argument and analysis theme across the essays.

```{r echo = FALSE, warning = FALSE, message = FALSE}
#word correlations
#new data object of tokens
data_two <- data %>%
  unnest_tokens(word, text)

#pairwise correlations
word_correlations <- data_two %>%
  group_by(word) %>%
  filter(n() >= 10) %>%
  pairwise_cor(word, url, sort = TRUE) %>%
  as_tibble()

#some correlations less than 0.99
#word_correlations %>%
#  filter(correlation < 0.99)


```

**Some example bigram correlations**

**Strong bigrams**

```{r echo = FALSE, warning = FALSE, message = FALSE}

#correlations with specific words
#strong
word_correlations %>%
  filter(item1 == 'strong')

```

**Pretty bigrams**

```{r echo = FALSE, warning = FALSE, message = FALSE}

#pretty
word_correlations %>%
  filter(item1 == 'pretty')

```

**Philosophical bigrams**

```{r echo = FALSE, warning = FALSE, message = FALSE}

#philosophical
word_correlations %>%
  filter(item1 == 'philosophical')


```

**Signal bigrams**

```{r echo = FALSE, warning = FALSE, message = FALSE}

#signal
word_correlations %>%
  filter(item1 == 'signal')

```

**Position, principle, religion, science**

```{r echo = FALSE, warning = FALSE, message = FALSE}

#visual of selected words and their correlations
#position, principal, religion, science
word_correlations %>%
  filter(item1 %in% c('position', 'principle', 'religion', 'science')) %>%
  group_by(item1) %>%
  top_n(5) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = 'identity', fill = '#528551') +
  xlab('Word') +
  ylab('Correlation') +
  labs(title = 'Selected Word Correlations') +
  facet_wrap(~ item1, scales = 'free') +
  coord_flip()

```

**Pairwise correlations of word relationships**

*Correlations greater than 0.85* 

```{r echo = FALSE, warning = FALSE, message = FALSE}

#relationship clusters with pairwise correlation
set.seed(1234)

word_correlations %>%
  filter(correlation > 0.85) %>%
  graph_from_data_frame() %>%
  ggraph(layout = 'fr') +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = 'red', size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

```


```{r echo = FALSE, warning = FALSE, message = FALSE}
#cleaning up work space
remove(bigrams_by_url, bigrams_separate, bigrams_tfidf, data_two, tokens,
       url_words, visual, word_correlations, words_by_url, bigrams_count,
       bigrams_visual)
```


```{r echo = FALSE, warning = FALSE, message = FALSE}
#topic modeling
text <- data$text %>%
  removePunctuation() %>%
  tolower() %>%
  removeWords(stopwords('smart')) %>%
  stripWhitespace()

#convert text vector to doc term matrix
term_matrix <- VCorpus(VectorSource(text)) %>%
  tm_map(stemDocument) %>%
  tm_map(stripWhitespace) %>%
  DocumentTermMatrix()

#first lda topic model
LDA_one <- LDA(term_matrix, k = 2, control = list(seed = 1234))

#per topic word probabilities
topics_one <- tidy(LDA_one, matrix = 'beta')
#View(topics_one)
```

```{r echo = FALSE, warning = FALSE, message = FALSE}
#sort by topic
top_topics <- topics_one %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

#visual of topics
top_topics %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  xlab('Word') +
  ylab('Beta Spread') +
  labs(title = 'Topic Modeling with beta spread',
       subtitle = '2 topics from LDA model') +
  facet_wrap(~ topic, scales = 'free') +
  coord_flip() +
  scale_x_reordered()

```

```{r echo = FALSE, warning = FALSE, message = FALSE}
#beta spread
beta_spread <- topics_one %>%
  mutate(topic = paste0('topic', topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > 0.001 | topic2 > 0.001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

#visual of greatest beta spreads
beta_spread %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_bar(stat = 'identity', fill = '#517985') +
  labs(title = 'Highest Beta Spreads',
       x = 'Term',
       y = 'Log Ratio') +
  coord_flip()

```

#### Topic modeling

**Identifying ideal number of topics**

```{r echo = FALSE, warning = FALSE, message = FALSE}
#calculate number of topics
topics_count <- FindTopicsNumber(term_matrix,
                                 topics = seq(from = 2, to = 20, by = 1),
                                 metrics = c('Griffiths2004',
                                             'CaoJuan2009',
                                             'Arun2010',
                                             'Devaud2014'),
                                 method = 'Gibbs',
                                 control = list(seed = 23),
                                 mc.cores = NA,
                                 verbose = TRUE)

#visual of results
FindTopicsNumber_plot(topics_count)

```

**Ideal number of topics appears to be 12, where Griffiths, CaoJuan, and Arun are closest.**

```{r echo = FALSE, warning = FALSE, message = FALSE}
#second lda topic model
LDA_two <- LDA(term_matrix, k = 12, control = list(seed = 1234))

#per topic word probabilities
topics_two <- tidy(LDA_two, matrix = 'beta')
#View(topics_two)

#sort by topic
top_topics_two <- topics_two %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

```

**Key words across the topic clusters**

```{r echo = FALSE, warning = FALSE, message = FALSE}

#visual of topics
top_topics_two %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  xlab('Term') +
  ylab('Beta spread') +
  facet_wrap(~ topic, scales = 'free') +
  coord_flip() +
  scale_x_reordered()

```

```{r echo = FALSE, warning = FALSE, message = FALSE}
#beta spread
beta_spread_two <- topics_two %>%
  mutate(topic = paste0('topic', topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > 0.001 | topic2 > 0.001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

#visual of greatest beta spreads
beta_spread_two %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_bar(stat = 'identity', fill = '#914161') +
  labs(title = 'Highest Beta Spreads',
       x = 'Term',
       y = 'Log Ratio') +
  coord_flip()

```

```{r echo = FALSE, warning = FALSE, message = FALSE}
#cleaning up work space
remove(beta_spread_two, beta_spread, LDA_one, LDA_two, term_matrix,
       top_topics, top_topics_two, topics_count, topics_one, topics_two)
```

#### Some simple word embeddings

```{r echo = FALSE, warning = FALSE, message = FALSE}
#word embeddings
#iterator object
tokens <- space_tokenizer(data$text)

#create vocabulary of unigrams
it <- itoken(tokens, progressbar = FALSE)
vocabulary <- create_vocabulary(it)

#reduce to words with minimum frequency of 5
vocabulary <- prune_vocabulary(vocabulary, term_count_min = 5L)
#length(vocabulary$term)

#term co-occurrence matrix
#vocabulary vectorizer object
vectorizer <- vocab_vectorizer(vocabulary)

#window for context words
matrix <- create_tcm(it, vocabulary, skip_grams_window = 5L)

```

**Model evaluations on each iteration**

```{r echo = FALSE, warning = FALSE, message = FALSE}
#fitting the model
glove <- GlobalVectors$new(rank = 50, x_max = 10)
model <- glove$fit_transform(matrix, n_iter = 10,
                             convergence_tol = 0.01,
                             n_threads = 4)

```

**Model dimensionality**

```{r echo = FALSE, warning = FALSE, message = FALSE}
#model dimensionality 
dim(model)

#sum of min and context vectors
context <- glove$components
vectors <- model + t(context)

```

**Some sample word context measures**

We can think of word embeddings as vectorized representations of text, in other words they are numerically expressed in a nested way. For example a given set of words will have a numerical value representing the weighted context across the sub-group it belongs to. In a sense, the sub-group of words are mapped to a vector and then numerically expressed by their co-occurrences. A group of words could be a group of paragraphs, while a sub-group could refer to each individual group of sentences within the given paragraph.

```{r echo = FALSE, warning = FALSE, message = FALSE}

#testing word contexts
sims <- sim2(x = vectors, y = vectors['smarter', , drop = FALSE],
             method = 'cosine', norm = 'l2')
head(sort(sims[,1], decreasing = TRUE), 5)

context_test <- vectors['smarter', , drop = FALSE] -
  vectors['arguments', , drop = FALSE] +
  vectors['economy', , drop = FALSE]

sims <- sim2(x = vectors, y = context_test,
             method = 'cosine', norm = 'l2')
head(sort(sims[,1], decreasing = TRUE), 5)
```

#### Why I chose this topic?

I read [LessWrong](https://www.lesswrong.com/) to keep in touch with rationality and philosophy as artificial intelligence and technology continue to advance at a rapid rate. I really enjoyed some economics courses in undergraduate, which introduced me briefly to the economics of human behavior, decision sciences, or behavioral economics. I also wanted to challenge myself by doing analytics on ambiguous text; text that contains many possible meanings, extends metaphorically across different examples, and applies in its own unique way from domain to domain.

#### Next steps

As in the 621 project, this was interesting in order to see some of the more challenging aspects of mining text and extracting meaning. I am interested in extracting logic from passages of text, especially those containing meta-laws or rule-based logic that is layered in a sense. I could see how this would go beyond the scope of a one-semester course and extend into a long-term research project. As for the project analysis itself, this could be extended to many different bodies of text and there will be a way to identify some key characteristics of passages. 

#### Last blurb

I learned how sentiment analysis is actually high-risk in a way; how it can't be relied on as a stand-alone form of text analysis, but rather needs to be augmented with other forms of analysis. An individual word without context could be represented in *any* way, the words in the corresponding group or sub-group of words provide meaning and lend a form of directionality to the initial word being observed.